# **特征工程**

《精通特征工程》学习笔记

**特征工程：**指从原始数据中提取特征并将其转换为适合机器学习模型的格式。

计数数据：

计数数据可以快速无限增长，并且数据大量且快速的生成时，很可能包含一些极端值，对于计数数据的处理包括：二值化，粗粒度分箱。

## 数值变量

1. 二值化

   在百万歌曲数据集中，原始的收听次数不是一个健壮的指标。不同用户的收听习惯不同，次数不能严格反映被多少听众播放过。更强壮的用户偏好表示方法是将次数二值化大于1的值都取1。即在阈值之上的数值记为1，反之记为0。

2. 分箱

   Yelp点评数据集中的点评数，大数部分商家的点评数很少，但有些商家的点评数有数千条。

   数量横跨了若干数量级，对于使用**欧式距离作为度量**的模型对于数据的数量级都非常敏感。比如 KNN，K-MEANS。一种解决方法是对数据分箱。

   2.1 固定宽度分箱

   区间的宽度固定。比如年龄常以10为宽度计数。当数值横跨多个量级时，最好用10的幂来分组。

   2.2 自适应分箱/分位数分箱

   固定宽度分箱易于计算，当计数值存在较大缺口，产生很多没有任何值的空箱。分位数分箱可以解决这一问	 题。分位数：将数据等分为若干份的值。排序后在（数组长度*X%）位置的数。中位数（二分位数）

3. 对数变换

   对于重尾分布的正数值数据，可以对数变换。

   重尾分布适用于对那些离峰值较远的**稀有事件也会有相当的概率发生**的情况。三个重要的子类别，分别是肥尾分布（Fat-tailed distribution），长尾分布（Long-tailed distribution）和次指数分布。

   适用于指数分布的数据，对数化可以近似视为线性。

   *对数变换不一定都会起积极作用。要结合实际。

4. 指数变换

   对数变换是指数变换的特例。都是**方差稳定性**变换。比如泊松分布作为一种重尾分布，它的方差等于均值。指数变换使得方差不再依赖于均值。

5. 特征缩放/归一化

   有些特征是有界的比如年龄，有些数值型特征可以无限制增加。**线性模型和基于欧式距离**的模型受到输入尺度的影响大。相反基于树的模型不在乎输入尺度。

   因为许多学习算法中目标函数的基础都是假设所有的**特征都是零均值**并且具有同一阶数上的**方差**(比如径向基函数、支持向量机以及**L1，L2正则化项**等)。如果某个特征的方差比其他特征大几个数量级，那么它就会在学习算法中占据主导位置，导致学习器并不能像我们说期望的那样，从其他特征中学习。

   如果模型对输入尺度敏感此时需要进行**特征缩放**。独立对每个特征维度进行缩放。

   缩放只改变数据的尺度***不改变数据的分布**。

   5.1 min-max缩放
   Min-max将特征值重新映射到[0, 1]区间。

   ![img](assets/wps16.png)

   5.2 标准化/方差缩放/中心化

   ![img](assets/wps17.png)

   5.3 不要“中心化”稀疏数据

   在稀疏数据上执行min-max缩放和标准化一定要慎重。因为它们都会使原始特征值减去一个量，如果这个量不	是0，会使一个多数是0的稀疏特征向量变成密集特征向量。
   5.4 MaxAbsScaler

   ![img](assets/wps18.png)

   除以最大值将特征缩放到最大绝对值为1，不会平移/中心化数据，因此不会破坏稀疏矩阵。
   5.5 L2归一化

   ![img](assets/wps19.png)

6. 交互特征

   两个特征的乘积可以组成一对简单的交互特征。

   策略：1. 对所有特征构造交互对，使训练的时间复杂度从O(n)->O(n^2)，再通过特征选择减少特征量。2. 精心设计少量的交互特征。 

## 分类变量

1. One-hot
    One-hot使特征稀疏化，对于线性模型适当的稀疏化有利于使数据线性可分。
    而对于树模型，稀疏特征的划分增益减小，使特征重要性下降。
    k个类别，映射到k维向量，1代表为某个类别。

2. Dummy encode
使K个类别映射到k-1维向量，全零向量也表示一个类别，称为参照类。

3. 效果编码
    同样将k个类别映射到k-1维，但参考类由分量全部为-1的向量编码。

4. 三种编码方式的对比
    One-hot有冗余，这会使得同一个问题有多个有效解，优点是每个特征对应一个类别。可以把缺失数据编码为零向量。
    虚拟编码没有冗余，可以生成唯一的可解释模型。但无法处理缺失值。
    效果编码没有冗余，可以生成唯一的可解释模型。可以处理缺失值，但是全部由-1组成的向量是密集向量，不利于存储和计算。
    以上三种编码方式均不适合处理超大型分类变量。

5. Label encode
    相同类别的特征编码成同一个值，所以最后编码的特征值是在[0, k]之间的整数

6. 大型分类变量

  6.1 特征散列化/hash
  使用hash函数把特征均匀映射到m维的空间。如果模型涉及特征向量和系数的内积运算，那么就可以使用特征散列化。比如线性模型和核方法。缺点，失去了可解释性。
  6.2 分箱计数
  按标签/某个维度的特征值，对数据分组，计算相应的条件概率作为特征值。
  可以构造odd优势比，或者对数优势比。
  6.3 Target encode

  ![img](assets/wps20.png)

  以**分类变量值**为k时的对应标签的**均值**替代k，原始的编码方式会发生数据泄露，导致过拟合。

  为了减轻过拟合：

  ![img](assets/wps21.png)

  α：正则项，p：所有标签的均值。

  希望同一个类别在不同的数据集中能够保持一致的与y之间的关联关系，显然很多时候训练集和测试集的**分布是存在一定差异**的，因此使用target encoding会引入一定偏差。

  target encoding有很多变种。关于类别变量的其他处理可以参考：[Category Encoders](http://contrib.scikit-learn.org/category_encoders/index.html)

## 特征选择

特征选择是一个很重要的数据预处理过程。为什么要进行特征选择？

1. 维度灾难问题
   1.1 特征数量越多，训练样本就会越稀疏，分类器的参数估计就会越不准确，更加容易出现过拟合问题。
   1.2 “维数灾难”的另一个影响是训练样本的稀疏性并**不是均匀分布**的。处于中心位置的训练样本比四周的训练样本更加稀疏。高维空间中，大多数的训练样本位于超立方体的角落。
2. 去除不相关特征往往会降低学习任务的难度

无关特征：与当前学习任务无关的特征。
冗余特征：指其包含的信息能从其他特征中推演出来。比如面积， 对于长和宽来说是冗余特征。冗余特征在很多时候不起作用，去除它们减轻学习任务的负担。但有当冗余特征恰好对应了完成学习任务所需的“中间概念”时是有用的。比如任务是学习体积时，面积就是一个有用的特征。

在此假设数据不涉及冗余特征，且包括了所有重要的信息。

欲从初始特征集合选出一个包含了所有重要的特征的子集，遍历所有的组合显然是不现实的。可行的做法是产生一个“候选子集”，评价其好坏，再基于评价结果产生下一个候选子集。直到无法找到更好的候选子集。

1. 子集搜索
   基于贪心的策略。存在实际最优组合不是每轮的最优组合的情况。
   1.1 前向搜索
   从{a1, a2, .......ap}的特征集合中选出一个最优的特征{ai}，第二轮再从{a1, a2, .......ap}-{ai}的子集中找出最优的{ai, aj}的组合。如此重复直到第k次的最优组合仍然低于k-1次的组合时停止。将k-1轮的组合认为是最优组合。
   1.2 后向搜索
   类似前向搜索。每次从里面删除一个特征。
   1.3 双向搜索
   在增加新特征的同时（新特征在后续不会被剔除），减少无关特征。
2. 子集评价
   假设特征均为离散变量。可以用Gain(A)（信息增益）评价特征的好坏。更一般的特征子集A实际上是对数据集D的划分，Y是对D的真实划分，能评价A的划分与Y的划分的差异的机制都能用于特征评价。二者差异越小越好。

将子集搜索与子集评价结合即可得到特征选择方法。

常见的特征选择方法大致分为：**过滤式，包裹式，嵌入式**。

### 过滤式

特征选择过程与后续学习器无关。
Relief: 著名的特征选择方法。
算法：算出每个特征对应的“相关统计量”的值，根据阈值过滤小于阈值的特征，或选择前k大的特征。
从给定训练集对于每个样本xi，先在xi的同类样本中寻找最近邻xi，nh称为“猜中最近邻”，在异类样本同样寻找最近邻xi，nm“猜错最近邻”。

统计量对应属性j的分量：

![img](assets/wps14.png)

当特征是离散值时diff函数的值取0/1，输入两值相等时取0，否则取1。
连续变量时<img src="assets/wps15.png" alt="img" style="zoom:80%;" />

* 特征j的值 = Σ样本xi与猜错近邻在j的距离—样本i与猜中近邻在j的距离
  如果是正值说明特征j对区分同类和异类有帮助。

实际上Relief算法对样本集采样进行相关计算，因此是一个运行效率高的过滤算法。多分类也有推广。
其他过滤指标：方差，卡方检验，person相关系数，信息增益，MIC。

### 包裹式

直接把最终的学习器的性能作为评价指标，由于包裹法直接针对给定学习器进行优化，因为包裹法要比过滤法更好。但计算开销大。

RFE递归特征消除：
从全部的特征集合开始，每次选出贡献最差的k个特征（步长）剔除，然后在剩余特征上继续重复这个过程，直到符合终止条件。采用了贪心法则。为了增强稳定性，这里模型评估常采用交叉验证的方法。需要模型给出特征重要性的评分，对于线性模型是系数，对于其他模型是feature_importance。

### 嵌入式

指将特征选择过程与学习器训练过程融为一体，二者在**同一个优化过程**中完成。即学习器训练过程中自动进行了特征选择。
以线性回归为例：
损失函数为MSE时，加上正则项构成目标函数。

![img](assets/wps11.png)

当使用L1正则时，叫做LASSO，使用L1范数的一个好处是，相较于L2范数，其求得的ω会有更少的非零分量（“稀疏”解）。

以X=(x1,x2)，ω=（ω1,ω2）为例，以ω为坐标轴，画出损失函数的等值线（在（ω1,ω2）空间内使损失函数的值相等的点连成的线），以及L1范数，L2范数的等值线（在（ω1,ω2）空间内使L1范数的值相等的点连成的线和使L2范数的值相等的点连成的线）。
采用L1范数时，正则项的等值线与损失函数的等值线的交点常出现在**坐标轴**上，即对应ω的**分量为零**，而L2的交点常在某一象限内，对应分量不为零。

ω取得稀疏解意味着仅ω对应分量不为零的特征出现在最终的模型。
换而言之使用L1正则化的学习方法就是一种嵌入式的特征选择方法。

### 参考

《机器学习》周志华

《精通特征工程》爱丽丝·郑 阿曼达·卡萨丽